{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afed4fb-f04b-4ac7-87cd-237fe854460c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4934423123080368>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmount\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwasbs://hospitalcontainer@srinihospitalpoc.blob.core.windows.net/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmount_point\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/mnt/bronze-mount\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextra_configs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfs.azure.account.key.srinihospitalpoc.blob.core.windows.net\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtXC2UPv9iqdN0+msHmU0LH7Y3LEabOp214cpFpxs7MDIKe2wb3BJg5OgGTXWU2AKDP2MS7ffx8Te+ASt+Oj62Q==\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n",
       "\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:364\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    362\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m    363\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[0;32m--> 364\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n",
       "\n",
       "\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o420.mount.\n",
       ": java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-mount; nested exception is: \n",
       "\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-mount\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1053)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1079)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n",
       "\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1073)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-mount\n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n",
       "\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.CEMountHandler.receive(MountHandler.scala:180)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n",
       "\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
       "\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1101)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1101)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21(DbfsServerBackend.scala:348)\n",
       "\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21$adapted(DbfsServerBackend.scala:347)\n",
       "\tat com.databricks.rpc.armeria.LegacyJettyCompatibilityWrapperBlocking.jettyHandler(CompatServerBackend.scala:492)\n",
       "\tat com.databricks.rpc.armeria.RPCContextCapturingPartialFunctionForLegacyRpc.apply(CompatServerBackend.scala:551)\n",
       "\tat com.databricks.rpc.armeria.RPCContextCapturingPartialFunctionForLegacyRpc.apply(CompatServerBackend.scala:544)\n",
       "\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n",
       "\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n",
       "\tat com.databricks.rpc.armeria.RPCContextCapturingPartialFunctionForLegacyRpc.applyOrElse(CompatServerBackend.scala:544)\n",
       "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:189)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:215)\n",
       "\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:215)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:186)\n",
       "\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$5(ServerBackend.scala:175)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n",
       "\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:23)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n",
       "\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:23)\n",
       "\tat com.databricks.rpc.ServerBackend.executeWithLogging$1(ServerBackend.scala:148)\n",
       "\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:175)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:997)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:917)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:557)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:522)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1099)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:64)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1099)\n",
       "\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1061)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1042)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$39(ActivityContextFactory.scala:409)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:64)\n",
       "\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:409)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)\n",
       "\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:417)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n",
       "\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n",
       "\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n",
       "\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n",
       "\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n",
       "\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n",
       "\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n",
       "\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n",
       "\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n",
       "\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n",
       "\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n",
       "\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n",
       "\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n",
       "\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n",
       "\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n",
       "\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n",
       "\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n",
       "\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n",
       "\tat java.lang.Thread.run(Thread.java:840)\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mExecutionError\u001B[0m                            Traceback (most recent call last)\nFile \u001B[0;32m<command-4934423123080368>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdbutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmount\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwasbs://hospitalcontainer@srinihospitalpoc.blob.core.windows.net/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmount_point\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/mnt/bronze-mount\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mextra_configs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfs.azure.account.key.srinihospitalpoc.blob.core.windows.net\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtXC2UPv9iqdN0+msHmU0LH7Y3LEabOp214cpFpxs7MDIKe2wb3BJg5OgGTXWU2AKDP2MS7ffx8Te+ASt+Oj62Q==\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/python_shell/dbruntime/dbutils.py:364\u001B[0m, in \u001B[0;36mDBUtils.FSHandler.prettify_exception_message.<locals>.f_with_exception_handling\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    362\u001B[0m exc\u001B[38;5;241m.\u001B[39m__context__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    363\u001B[0m exc\u001B[38;5;241m.\u001B[39m__cause__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 364\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exc\n\n\u001B[0;31mExecutionError\u001B[0m: An error occurred while calling o420.mount.\n: java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-mount; nested exception is: \n\tjava.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-mount\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:135)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:69)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.createOrUpdateMount(DBUtilsCore.scala:1053)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.$anonfun$mount$1(DBUtilsCore.scala:1079)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:657)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:678)\n\tat com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:414)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:158)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:412)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:409)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:457)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:442)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:652)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:569)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:71)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:560)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:528)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:71)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:135)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:1073)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-mount\n\tat scala.Predef$.require(Predef.scala:281)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$insertMount$1(MetadataManager.scala:930)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.$anonfun$modifyAndVerify$2(MetadataManager.scala:1312)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.withRetries(MetadataManager.scala:1085)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.modifyAndVerify(MetadataManager.scala:1301)\n\tat com.databricks.backend.daemon.data.server.DefaultMetadataManager.insertMount(MetadataManager.scala:938)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:136)\n\tat com.databricks.backend.daemon.data.server.handler.CEMountHandler.receive(MountHandler.scala:180)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive(DbfsRequestHandler.scala:16)\n\tat com.databricks.backend.daemon.data.server.handler.DbfsRequestHandler.receive$(DbfsRequestHandler.scala:15)\n\tat com.databricks.backend.daemon.data.server.handler.MountHandler.receive(MountHandler.scala:40)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1(SessionContext.scala:52)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.$anonfun$queryHandlers$1$adapted(SessionContext.scala:51)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat com.databricks.backend.daemon.data.server.session.SessionContext.queryHandlers(SessionContext.scala:51)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$handleOtherRpc$3(DbfsServerBackend.scala:1101)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.handleOtherRpc(DbfsServerBackend.scala:1101)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21(DbfsServerBackend.scala:348)\n\tat com.databricks.backend.daemon.data.server.DbfsServerBackend.$anonfun$new$21$adapted(DbfsServerBackend.scala:347)\n\tat com.databricks.rpc.armeria.LegacyJettyCompatibilityWrapperBlocking.jettyHandler(CompatServerBackend.scala:492)\n\tat com.databricks.rpc.armeria.RPCContextCapturingPartialFunctionForLegacyRpc.apply(CompatServerBackend.scala:551)\n\tat com.databricks.rpc.armeria.RPCContextCapturingPartialFunctionForLegacyRpc.apply(CompatServerBackend.scala:544)\n\tat scala.PartialFunction.applyOrElse(PartialFunction.scala:127)\n\tat scala.PartialFunction.applyOrElse$(PartialFunction.scala:126)\n\tat com.databricks.rpc.armeria.RPCContextCapturingPartialFunctionForLegacyRpc.applyOrElse(CompatServerBackend.scala:544)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive0$2(ServerBackend.scala:189)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:215)\n\tat com.databricks.rpc.ServerBackend$$anonfun$commonReceive$1.applyOrElse(ServerBackend.scala:215)\n\tat com.databricks.rpc.ServerBackend.internalReceive0(ServerBackend.scala:186)\n\tat com.databricks.rpc.ServerBackend.$anonfun$internalReceive$5(ServerBackend.scala:175)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.rpc.ServerBackend.withAttributionContext(ServerBackend.scala:23)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)\n\tat com.databricks.rpc.ServerBackend.withAttributionTags(ServerBackend.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)\n\tat com.databricks.rpc.ServerBackend.recordOperationWithResultTags(ServerBackend.scala:23)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)\n\tat com.databricks.rpc.ServerBackend.recordOperation(ServerBackend.scala:23)\n\tat com.databricks.rpc.ServerBackend.executeWithLogging$1(ServerBackend.scala:148)\n\tat com.databricks.rpc.ServerBackend.internalReceive(ServerBackend.scala:175)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRPC(JettyServer.scala:997)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleRequestAndRespond(JettyServer.scala:917)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5(JettyServer.scala:557)\n\tat com.databricks.rpc.JettyServer$RequestManager.$anonfun$handleHttp$5$adapted(JettyServer.scala:522)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$12(ActivityContextFactory.scala:1099)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:64)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withActivityInternal$2(ActivityContextFactory.scala:1099)\n\tat com.databricks.context.integrity.IntegrityCheckContext$ThreadLocalStorage$.withValue(IntegrityCheckContext.scala:73)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1061)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withActivityInternal(ActivityContextFactory.scala:1042)\n\tat com.databricks.logging.activity.ActivityContextFactory$.$anonfun$withServiceRequestActivity$39(ActivityContextFactory.scala:409)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withAttributionContext(ActivityContextFactory.scala:64)\n\tat com.databricks.logging.activity.ActivityContextFactory$.withServiceRequestActivity(ActivityContextFactory.scala:409)\n\tat com.databricks.rpc.JettyServer$RequestManager.handleHttp(JettyServer.scala:522)\n\tat com.databricks.rpc.JettyServer$RequestManager.doPost(JettyServer.scala:417)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:665)\n\tat com.databricks.rpc.HttpServletWithPatch.service(HttpServletWithPatch.scala:33)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:750)\n\tat org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:799)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:554)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:190)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:505)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:127)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:516)\n\tat org.eclipse.jetty.server.HttpChannel.lambda$handle$1(HttpChannel.java:487)\n\tat org.eclipse.jetty.server.HttpChannel.dispatch(HttpChannel.java:732)\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:479)\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:277)\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:311)\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:105)\n\tat org.eclipse.jetty.io.ChannelEndPoint$1.run(ChannelEndPoint.java:104)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.runTask(EatWhatYouKill.java:338)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:315)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.tryProduce(EatWhatYouKill.java:173)\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:409)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$2(InstrumentedQueuedThreadPool.scala:111)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.withAttributionContext(InstrumentedQueuedThreadPool.scala:46)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.$anonfun$run$1(InstrumentedQueuedThreadPool.scala:111)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads(QueuedThreadPoolInstrumenter.scala:132)\n\tat com.databricks.instrumentation.QueuedThreadPoolInstrumenter.trackActiveThreads$(QueuedThreadPoolInstrumenter.scala:129)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool.trackActiveThreads(InstrumentedQueuedThreadPool.scala:46)\n\tat com.databricks.rpc.InstrumentedQueuedThreadPool$$anon$1.run(InstrumentedQueuedThreadPool.scala:93)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:883)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:1034)\n\tat java.lang.Thread.run(Thread.java:840)\n",
       "errorSummary": "java.rmi.RemoteException: java.lang.IllegalArgumentException: requirement failed: Directory already mounted: /mnt/bronze-mount; nested exception is: ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.mount(\n",
    "    source=\"wasbs://hospitalcontainer@srinihospitalpoc.blob.core.windows.net/\",\n",
    "    mount_point=\"/mnt/bronze-mount\",\n",
    "    extra_configs={\n",
    "        \"fs.azure.account.key.srinihospitalpoc.blob.core.windows.net\": \"tXC2UPv9iqdN0+msHmU0LH7Y3LEabOp214cpFpxs7MDIKe2wb3BJg5OgGTXWU2AKDP2MS7ffx8Te+ASt+Oj62Q==\"\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45b7c7d0-5552-4ece-826a-8813f1aec44f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mountPoint</th><th>source</th><th>encryptionType</th></tr></thead><tbody><tr><td>/databricks-datasets</td><td>databricks-datasets</td><td></td></tr><tr><td>/mnt/bronze-srini</td><td>wasbs://srini-adlsgen2adventureworks@adlsgen2adventureworks.blob.core.windows.net/</td><td></td></tr><tr><td>/databricks/mlflow-tracking</td><td>databricks/mlflow-tracking</td><td>sse-s3</td></tr><tr><td>/databricks-results</td><td>databricks-results</td><td>sse-s3</td></tr><tr><td>/databricks/mlflow-registry</td><td>databricks/mlflow-registry</td><td>sse-s3</td></tr><tr><td>/mnt/bronze-mount</td><td>wasbs://hospitalcontainer@srinihospitalpoc.blob.core.windows.net/</td><td></td></tr><tr><td>/</td><td>DatabricksRoot</td><td>sse-s3</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "/databricks-datasets",
         "databricks-datasets",
         ""
        ],
        [
         "/mnt/bronze-srini",
         "wasbs://srini-adlsgen2adventureworks@adlsgen2adventureworks.blob.core.windows.net/",
         ""
        ],
        [
         "/databricks/mlflow-tracking",
         "databricks/mlflow-tracking",
         "sse-s3"
        ],
        [
         "/databricks-results",
         "databricks-results",
         "sse-s3"
        ],
        [
         "/databricks/mlflow-registry",
         "databricks/mlflow-registry",
         "sse-s3"
        ],
        [
         "/mnt/bronze-mount",
         "wasbs://hospitalcontainer@srinihospitalpoc.blob.core.windows.net/",
         ""
        ],
        [
         "/",
         "DatabricksRoot",
         "sse-s3"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "mountPoint",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "encryptionType",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/bronze-mount/Bronze/Encounters/</td><td>Encounters/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/bronze-mount/Bronze/Organizations/</td><td>Organizations/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/bronze-mount/Bronze/Patients/</td><td>Patients/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/bronze-mount/Bronze/Payers/</td><td>Payers/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/bronze-mount/Bronze/Procedures/</td><td>Procedures/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/bronze-mount/Bronze/Encounters/",
         "Encounters/",
         0,
         0
        ],
        [
         "dbfs:/mnt/bronze-mount/Bronze/Organizations/",
         "Organizations/",
         0,
         0
        ],
        [
         "dbfs:/mnt/bronze-mount/Bronze/Patients/",
         "Patients/",
         0,
         0
        ],
        [
         "dbfs:/mnt/bronze-mount/Bronze/Payers/",
         "Payers/",
         0,
         0
        ],
        [
         "dbfs:/mnt/bronze-mount/Bronze/Procedures/",
         "Procedures/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List all mounted points\n",
    "display(dbutils.fs.mounts())\n",
    "\n",
    "display(dbutils.fs.ls(\"/mnt/bronze-mount/Bronze/\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae1eabe-c7e2-44df-8875-1c141dc38f87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Encounters data from: /mnt/bronze-mount/Bronze/Encounters/2025/11/08/ | 27891 records\nLoaded Organizations data from: /mnt/bronze-mount/Bronze/Organizations/2025/11/07/ | 1 records\nLoaded Patients data from: /mnt/bronze-mount/Bronze/Patients/2025/11/07/ | 974 records\nLoaded Payers data from: /mnt/bronze-mount/Bronze/Payers/2025/11/07/ | 10 records\nLoaded Procedures data from: /mnt/bronze-mount/Bronze/Procedures/2025/11/08/ | 47701 records\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Base path\n",
    "base_path = \"/mnt/bronze-mount/Bronze\"\n",
    "\n",
    "def get_latest_folder(base_path: str):\n",
    "    \"\"\"\n",
    "    Returns the latest (year/month/day) folder path dynamically from a given base folder.\n",
    "    Example: /mnt/bronze-mount/Bronze/Payers â†’ /mnt/bronze-mount/Bronze/Payers/2025/11/07/\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get available years\n",
    "        years = [f.name.replace(\"/\", \"\") for f in dbutils.fs.ls(base_path)]\n",
    "        latest_year = max(years)\n",
    "\n",
    "        # Get available months for that year\n",
    "        months = [f.name.replace(\"/\", \"\") for f in dbutils.fs.ls(f\"{base_path}/{latest_year}\")]\n",
    "        latest_month = max(months)\n",
    "\n",
    "        # Get available days for that month\n",
    "        days = [f.name.replace(\"/\", \"\") for f in dbutils.fs.ls(f\"{base_path}/{latest_year}/{latest_month}\")]\n",
    "        latest_day = max(days)\n",
    "\n",
    "        # Build final path\n",
    "        latest_path = f\"{base_path}/{latest_year}/{latest_month}/{latest_day}/\"\n",
    "        return latest_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error while finding latest folder: {e}\")\n",
    "        return None\n",
    "entities = [f.name.replace(\"/\", \"\") for f in dbutils.fs.ls(base_path)]\n",
    "for entity in entities:\n",
    "    entity_path = f\"{base_path}/{entity}\"\n",
    "    latest_path = get_latest_folder(entity_path)\n",
    "    \n",
    "    if latest_path:\n",
    "        df = spark.read.parquet(latest_path)\n",
    "        print(f\"Loaded {entity} data from: {latest_path} | {df.count()} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8d44a78-9395-4c09-8290-8f1ed2c7704f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning Encounters data from /mnt/bronze-mount/Bronze/Encounters/2025/11/08/Encounters.parquet\n Cleaned Encounters | Records: 27891\n Encounters Delta table saved to /mnt/bronze-mount/Silver/Encounters\nCleaning Organizations data from /mnt/bronze-mount/Bronze/Organizations/2025/11/07/Organizations.parquet\n Cleaned Organizations | Records: 1\n Organizations Delta table saved to /mnt/bronze-mount/Silver/Organizations\n Cleaning Patients data from /mnt/bronze-mount/Bronze/Patients/2025/11/07/Patients.parquet\n Cleaned Patients | Records: 974\n Patients Delta table saved to /mnt/bronze-mount/Silver/Patients\n Cleaning Payers data from /mnt/bronze-mount/Bronze/Payers/2025/11/07/Payers.parquet\n Cleaned Payers | Records: 10\n Payers Delta table saved to /mnt/bronze-mount/Silver/Payers\n Cleaning Procedures data from /mnt/bronze-mount/Bronze/Procedures/2025/11/08/Procedures.parquet\n Cleaned Procedures | Records: 47701\n Procedures Delta table saved to /mnt/bronze-mount/Silver/Procedures\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col, trim, regexp_replace, when, length, lpad\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"BronzeToSilver_Cleaning\").getOrCreate()\n",
    "\n",
    "# Base Bronze & Silver paths\n",
    "bronze_base_path = \"/mnt/bronze-mount/Bronze\"\n",
    "silver_base_path = \"/mnt/bronze-mount/Silver\"\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Function: Get latest (Year/Month/Day) folder in ADLS\n",
    "# ---------------------------------------------------------\n",
    "def get_latest_folder(base_path: str):\n",
    "    try:\n",
    "        years = [f.name.replace(\"/\", \"\") for f in dbutils.fs.ls(base_path)]\n",
    "        latest_year = max(years)\n",
    "\n",
    "        months = [f.name.replace(\"/\", \"\") for f in dbutils.fs.ls(f\"{base_path}/{latest_year}\")]\n",
    "        latest_month = max(months)\n",
    "\n",
    "        days = [f.name.replace(\"/\", \"\") for f in dbutils.fs.ls(f\"{base_path}/{latest_year}/{latest_month}\")]\n",
    "        latest_day = max(days)\n",
    "\n",
    "        latest_path = f\"{base_path}/{latest_year}/{latest_month}/{latest_day}/\"\n",
    "        return latest_path\n",
    "    except Exception as e:\n",
    "        print(f\" Error locating latest folder in {base_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Iterate over all entities under Bronze\n",
    "# ---------------------------------------------------------\n",
    "entities = [f.name.replace(\"/\", \"\") for f in dbutils.fs.ls(bronze_base_path)]\n",
    "\n",
    "for entity in entities:\n",
    "    entity_path = f\"{bronze_base_path}/{entity}\"\n",
    "    latest_path = get_latest_folder(entity_path)\n",
    "\n",
    "    if not latest_path:\n",
    "        print(f\"No valid folder found for {entity}\")\n",
    "        continue\n",
    "\n",
    "    files = [f.name for f in dbutils.fs.ls(latest_path)]\n",
    "\n",
    "    for file_name in files:\n",
    "        file_name_clean = file_name.replace(\".Parquet\", \"\").replace(\".parquet\", \"\")\n",
    "        file_path = latest_path + file_name\n",
    "\n",
    "        # ====================================================\n",
    "        #  Entity-specific cleaning logic\n",
    "        # ====================================================\n",
    "\n",
    "        # -------------------------------\n",
    "        #  PATIENTS\n",
    "        # -------------------------------\n",
    "        if file_name_clean.lower() == \"patients\":\n",
    "            print(f\" Cleaning Patients data from {file_path}\")\n",
    "            df = spark.read.parquet(file_path)\n",
    "\n",
    "            # 1ï¸ Standardize column names\n",
    "            for c in df.columns:\n",
    "                df = df.withColumnRenamed(c, c.lower().strip().replace(\" \", \"_\"))\n",
    "\n",
    "            # 2ï¸ Trim all string columns\n",
    "            string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "            for c in string_cols:\n",
    "                df = df.withColumn(c, F.trim(F.col(c)))\n",
    "\n",
    "            # 3ï¸ Clean ZIP codes\n",
    "            if \"zip\" in df.columns:\n",
    "                df = df.withColumn(\"zip\", F.regexp_replace(F.col(\"zip\").cast(\"string\"), r\"\\.0$\", \"\"))\n",
    "                df = df.withColumn(\"zip\", F.when(F.length(F.col(\"zip\")) < 5, F.lpad(F.col(\"zip\"), 5, \"0\")).otherwise(F.col(\"zip\")))\n",
    "                df = df.withColumn(\"zip\", F.when(F.col(\"zip\").isNull() | (F.trim(F.col(\"zip\")) == \"\"), \"00000\").otherwise(F.col(\"zip\")))\n",
    "\n",
    "            # 4ï¸ Normalize name-related columns\n",
    "            for c in [\"prefix\", \"first\", \"last\", \"maiden\", \"suffix\"]:\n",
    "                if c in df.columns:\n",
    "                    df = df.withColumn(c, F.initcap(F.trim(F.col(c))))\n",
    "                    default_value = \"N/A\" if c in [\"suffix\", \"maiden\"] else \"Not Provided\"\n",
    "                    df = df.withColumn(c, F.when(F.col(c).isNull() | (F.trim(F.col(c)) == \"\"), default_value).otherwise(F.col(c)))\n",
    "\n",
    "            # 5ï¸ Standardize categorical columns\n",
    "            for c in [\"marital\", \"race\", \"ethnicity\", \"gender\"]:\n",
    "                if c in df.columns:\n",
    "                    df = df.withColumn(c, F.upper(F.trim(F.col(c))))\n",
    "                    df = df.withColumn(c, F.when(F.col(c).isNull() | (F.col(c) == \"\"), \"UNKNOWN\").otherwise(F.col(c)))\n",
    "\n",
    "            # 6ï¸ Convert BIRTHDATE/DEATHDATE to timestamp and compute AGE\n",
    "            if \"birthdate\" in df.columns:\n",
    "                df = df.withColumn(\"birthdate\", F.to_timestamp(\"birthdate\", \"yyyy-MM-dd\"))\n",
    "                df = df.withColumn(\"age\", F.floor(F.datediff(F.current_date(), F.col(\"birthdate\")) / 365.25))\n",
    "            if \"deathdate\" in df.columns:\n",
    "                df = df.withColumn(\"deathdate\", F.to_timestamp(\"deathdate\", \"yyyy-MM-dd\"))\n",
    "\n",
    "            # 7ï¸ Clean address/location fields\n",
    "            for c in [\"address\", \"city\", \"state\", \"county\", \"birthplace\"]:\n",
    "                if c in df.columns:\n",
    "                    df = df.withColumn(c, F.initcap(F.trim(F.regexp_replace(F.col(c), r\"\\s+\", \" \"))))\n",
    "                    df = df.withColumn(c, F.when(F.col(c).isNull() | (F.col(c) == \"\"), \"Unknown\").otherwise(F.col(c)))\n",
    "\n",
    "            # 8ï¸ Validate coordinates\n",
    "            for c in [\"lat\", \"lon\"]:\n",
    "                if c in df.columns:\n",
    "                    df = df.withColumn(c, F.when(F.col(c).isNull(), 0.0).otherwise(F.col(c)))\n",
    "\n",
    "            \n",
    "\n",
    "            #  Remove non-printable characters\n",
    "            for c in string_cols:\n",
    "                df = df.withColumn(c, F.regexp_replace(F.col(c), \"[^\\\\x20-\\\\x7E]\", \"\")) \n",
    "            df = df.withColumn(\"Last_Modified\", F.current_timestamp())    \n",
    "\n",
    "            df_Patients = df\n",
    "            print(f\" Cleaned Patients | Records: {df_Patients.count()}\")\n",
    "\n",
    "            #  Write to Silver Delta table\n",
    "            silver_path = f\"{silver_base_path}/Patients\"\n",
    "            df_Patients.write.format(\"delta\").mode(\"append\").save(silver_path)\n",
    "            print(f\" Patients Delta table saved to {silver_path}\")\n",
    "\n",
    "        # -------------------------------\n",
    "        #  PAYERS\n",
    "        # -------------------------------\n",
    "        elif file_name_clean.lower() == \"payers\":\n",
    "            print(f\" Cleaning Payers data from {file_path}\")\n",
    "            df = spark.read.parquet(file_path)\n",
    "\n",
    "            for c in df.columns:\n",
    "                df = df.withColumnRenamed(c, c.upper())\n",
    "\n",
    "            string_cols = [f.name for f in df.schema.fields if f.dataType == StringType()]\n",
    "            for c in string_cols:\n",
    "                df = df.withColumn(c, trim(col(c)))\n",
    "\n",
    "            if \"ZIP\" in df.columns:\n",
    "                df = df.withColumn(\"ZIP\", regexp_replace(col(\"ZIP\").cast(StringType()), r\"\\.0$\", \"\"))\n",
    "                df = df.withColumn(\"ZIP\", when(length(col(\"ZIP\")) < 5, lpad(col(\"ZIP\"), 5, \"0\")).otherwise(col(\"ZIP\")))\n",
    "                df = df.withColumn(\"ZIP\", when(col(\"ZIP\").isNull(), \"00000\").otherwise(col(\"ZIP\")))\n",
    "\n",
    "            if \"PHONE\" in df.columns:\n",
    "                df = df.withColumn(\"PHONE\", regexp_replace(col(\"PHONE\").cast(StringType()), r\"[^0-9]\", \"\"))\n",
    "                df = df.withColumn(\"PHONE\", when(col(\"PHONE\") == \"\", \"Not Provided\").otherwise(col(\"PHONE\")))\n",
    "\n",
    "            fill_defaults = {\n",
    "                \"ADDRESS\": \"Not Provided\",\n",
    "                \"CITY\": \"Unknown\",\n",
    "                \"STATE_HEADQUARTERED\": \"Unknown\",\n",
    "                \"PHONE\": \"Not Provided\",\n",
    "                \"ZIP\": \"00000\"\n",
    "            }\n",
    "            for k, v in fill_defaults.items():\n",
    "                if k in df.columns:\n",
    "                    df = df.withColumn(k, when(col(k).isNull() | (trim(col(k)) == \"\"), v).otherwise(col(k)))\n",
    "\n",
    "            if \"NAME\" in df.columns:\n",
    "                df = df.withColumn(\"NAME\", regexp_replace(trim(col(\"NAME\")), r\"\\s+\", \" \"))\n",
    "            if \"CITY\" in df.columns:\n",
    "                df = df.withColumn(\"CITY\", regexp_replace(trim(col(\"CITY\")), r\"\\s+\", \" \"))\n",
    "\n",
    "            \n",
    "\n",
    "            df_Payers = df\n",
    "            print(f\" Cleaned Payers | Records: {df_Payers.count()}\")\n",
    "\n",
    "            #  Write to Silver Delta table\n",
    "            silver_path = f\"{silver_base_path}/Payers\"\n",
    "            df_Payers.write.format(\"delta\").mode(\"append\").save(silver_path)\n",
    "            print(f\" Payers Delta table saved to {silver_path}\")\n",
    "\n",
    "        # -------------------------------\n",
    "        #  PROCEDURES\n",
    "        # -------------------------------\n",
    "        elif file_name_clean.lower() == \"procedures\":\n",
    "            print(f\" Cleaning Procedures data from {file_path}\")\n",
    "            df = spark.read.parquet(file_path)\n",
    "\n",
    "            for c in df.columns:\n",
    "                df = df.withColumnRenamed(c, c.lower().strip().replace(\" \", \"_\"))\n",
    "\n",
    "            df = df.withColumn(\"start\", F.to_timestamp(\"start\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\n",
    "                   .withColumn(\"stop\", F.to_timestamp(\"stop\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
    "            df = df.fillna({\"reasoncode\": 0, \"reasondescription\": \"Unknown Reason\"})\n",
    "\n",
    "            for c in [\"description\", \"reasondescription\"]:\n",
    "                if c in df.columns:\n",
    "                    df = df.withColumn(c, F.trim(F.initcap(F.col(c))))\n",
    "                    df = df.withColumn(c, F.regexp_replace(F.col(c), \"[^\\\\x20-\\\\x7E]\", \"\"))\n",
    "\n",
    "            if \"base_cost\" in df.columns:\n",
    "                df = df.withColumn(\"base_cost\", F.when(F.col(\"base_cost\") < 0, 0).otherwise(F.col(\"base_cost\")))\n",
    "\n",
    "            if all(col in df.columns for col in [\"start\", \"stop\"]):\n",
    "                df = df.withColumn(\"duration_minutes\", (F.col(\"stop\").cast(\"long\") - F.col(\"start\").cast(\"long\")) / 60)\n",
    "            df = df.withColumn(\"Procedure_Id\", F.monotonically_increasing_id())\n",
    "            df_Procedures = df\n",
    "            print(f\" Cleaned Procedures | Records: {df_Procedures.count()}\")\n",
    "            \n",
    "\n",
    "            #  Write to Silver Delta table\n",
    "            silver_path = f\"{silver_base_path}/Procedures\"\n",
    "            df_Procedures.write.format(\"delta\").mode(\"append\").save(silver_path)\n",
    "            print(f\" Procedures Delta table saved to {silver_path}\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # ENCOUNTERS\n",
    "        # -------------------------------\n",
    "        elif file_name_clean.lower() == \"encounters\":\n",
    "            print(f\" Cleaning Encounters data from {file_path}\")\n",
    "            df = spark.read.parquet(file_path)\n",
    "\n",
    "            for c in df.columns:\n",
    "                df = df.withColumnRenamed(c, c.lower().strip().replace(\" \", \"_\"))\n",
    "\n",
    "            df = df.withColumn(\"start\", F.to_timestamp(\"start\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\")) \\\n",
    "                   .withColumn(\"stop\", F.to_timestamp(\"stop\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
    "\n",
    "            fill_values = {\n",
    "                \"reasoncode\": 0,\n",
    "                \"reasondescription\": \"Unknown Reason\",\n",
    "                \"payer_coverage\": 0,\n",
    "                \"base_encounter_cost\": 0,\n",
    "                \"total_claim_cost\": 0\n",
    "            }\n",
    "            df = df.fillna(fill_values)\n",
    "\n",
    "            for c in [\"description\", \"reasondescription\"]:\n",
    "                if c in df.columns:\n",
    "                    df = df.withColumn(c, F.trim(F.initcap(F.col(c))))\n",
    "                    df = df.withColumn(c, F.regexp_replace(F.col(c), \"[^\\\\x20-\\\\x7E]\", \"\"))\n",
    "\n",
    "            if \"start\" in df.columns and \"stop\" in df.columns:\n",
    "                df = df.withColumn(\"duration_minutes\", (F.col(\"stop\").cast(\"long\") - F.col(\"start\").cast(\"long\")) / 60)\n",
    "\n",
    "            df_Encounters = df\n",
    "            print(f\" Cleaned Encounters | Records: {df_Encounters.count()}\")\n",
    "\n",
    "            #  Write to Silver Delta table\n",
    "            silver_path = f\"{silver_base_path}/Encounters\"\n",
    "            df_Encounters.write.format(\"delta\").mode(\"append\").save(silver_path)\n",
    "            print(f\" Encounters Delta table saved to {silver_path}\")\n",
    "\n",
    "        # -------------------------------\n",
    "        #  ORGANIZATIONS\n",
    "        # -------------------------------\n",
    "        elif file_name_clean.lower() == \"organizations\":\n",
    "            print(f\"Cleaning Organizations data from {file_path}\")\n",
    "            df = spark.read.parquet(file_path)\n",
    "\n",
    "            for c in df.columns:\n",
    "                df = df.withColumnRenamed(c, c.lower().strip().replace(\" \", \"_\"))\n",
    "\n",
    "            string_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "            for c in string_cols:\n",
    "                df = df.withColumn(c, F.trim(F.col(c)))\n",
    "\n",
    "            if \"zip\" in df.columns:\n",
    "                df = df.withColumn(\"zip\", F.col(\"zip\").cast(\"string\"))\n",
    "                df = df.withColumn(\"zip\", F.regexp_replace(F.col(\"zip\"), r\"\\.0$\", \"\"))\n",
    "                df = df.withColumn(\"zip\", F.when(F.length(F.col(\"zip\")) < 5, F.lpad(F.col(\"zip\"), 5, \"0\")).otherwise(F.col(\"zip\")))\n",
    "                df = df.withColumn(\"zip\", F.when(F.col(\"zip\").isNull() | (F.col(\"zip\") == \"\"), \"00000\").otherwise(F.col(\"zip\")))\n",
    "\n",
    "            if \"name\" in df.columns:\n",
    "                df = df.withColumn(\"name\", F.initcap(F.trim(F.regexp_replace(F.col(\"name\"), r\"\\s+\", \" \"))))\n",
    "                df = df.withColumn(\"name\", F.when(F.col(\"name\").isNull() | (F.col(\"name\") == \"\"), \"Unknown Organization\").otherwise(F.col(\"name\")))\n",
    "\n",
    "            df_Organizations = df\n",
    "            print(f\" Cleaned Organizations | Records: {df_Organizations.count()}\")\n",
    "\n",
    "            #  Write to Silver Delta table\n",
    "            silver_path = f\"{silver_base_path}/Organizations\"\n",
    "            df_Organizations.write.format(\"delta\").mode(\"append\").save(silver_path)\n",
    "            print(f\" Organizations Delta table saved to {silver_path}\")\n",
    "\n",
    "        else:\n",
    "            print(f\" No cleaning logic defined for {file_name_clean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef1670a-79a3-413f-a708-b9b691f8f847",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n==============================\nðŸš€ Incremental Load: Patients\n==============================\nðŸ•“ Last Watermark for Patients: 2010-01-01 00:00:00\nðŸ“… Incremental Column: Last_Modified\nâœ… Incremental Records Found: 974\nðŸ’¾ 974 records written to dbo.Patients_Prestage.\nâœ… Watermark updated for Patients â†’ 2025-11-10 20:15:06\n\n==============================\nðŸš€ Incremental Load: Procedures\n==============================\nðŸ•“ Last Watermark for Procedures: 2010-01-01 00:00:00\nðŸ“… Incremental Column: start\nâœ… Incremental Records Found: 47701\nðŸ’¾ 47701 records written to dbo.Procedures_Prestage.\nâœ… Watermark updated for Procedures â†’ 2022-01-30 02:05:37\n\n==============================\nðŸš€ Incremental Load: Encounters\n==============================\nðŸ•“ Last Watermark for Encounters: 2010-01-01 00:00:00\nðŸ“… Incremental Column: start\nâœ… Incremental Records Found: 27891\nðŸ’¾ 27891 records written to dbo.Encounters_Prestage.\nâœ… Watermark updated for Encounters â†’ 2022-02-06 01:57:36\n\n==============================\nðŸš€ Full Load: Payers\n==============================\nâœ… Total Records to Load: 10\nðŸ’¾ Full load completed for Payers: 10 records written.\n\n==============================\nðŸš€ Full Load: Organizations\n==============================\nâœ… Total Records to Load: 1\nðŸ’¾ Full load completed for Organizations: 1 records written.\n\nðŸŽ‰ All Silver tables processed successfully (Incremental + Full loads)!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0c0a1b5-7d21-4648-885e-f85fe6734474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Initialize Spark Session\n",
    "# ---------------------------------------------\n",
    "spark = SparkSession.builder.appName(\"SilverToSQL_Load_AllTables\").getOrCreate()\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Base Silver Path\n",
    "# ---------------------------------------------\n",
    "silver_base_path = \"/mnt/bronze-mount/Silver\"\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Azure SQL Connection Details\n",
    "# ---------------------------------------------\n",
    "sql_server_name = \"azuresqlansdb.database.windows.net\"\n",
    "sql_database_name = \"srinipocdb\"\n",
    "sql_user = \"ANS\"\n",
    "sql_password = \"@srinu1001\"\n",
    "\n",
    "jdbc_url = (\n",
    "    f\"jdbc:sqlserver://{sql_server_name}:1433;\"\n",
    "    f\"database={sql_database_name};\"\n",
    "    \"encrypt=true;\"\n",
    "    \"trustServerCertificate=false;\"\n",
    "    \"hostNameInCertificate=*.database.windows.net;\"\n",
    "    \"loginTimeout=30;\"\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Table Lists (by Load Type)\n",
    "# ---------------------------------------------\n",
    "incremental_tables = [\"Patients\", \"Procedures\", \"Encounters\"]\n",
    "full_load_tables = [\"Payers\", \"Organizations\"]\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Process Incremental Tables\n",
    "# ---------------------------------------------\n",
    "for table_name in incremental_tables:\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"ðŸš€ Incremental Load: {table_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    silver_path = f\"{silver_base_path}/{table_name}\"\n",
    "\n",
    "    # ðŸ•“ Step 1 â€” Get Last Watermark & Incremental Column\n",
    "    config_query = f\"\"\"\n",
    "    SELECT LastWatermarkValue, IncrementalColumn\n",
    "    FROM dbo.configtable\n",
    "    WHERE SourceObjectName = '{table_name}'\n",
    "      AND DestinationZone = 'Goldprestage'\n",
    "    \"\"\"\n",
    "\n",
    "    watermark_df = (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", jdbc_url)\n",
    "        .option(\"query\", config_query)\n",
    "        .option(\"user\", sql_user)\n",
    "        .option(\"password\", sql_password)\n",
    "        .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "    if watermark_df.count() == 0:\n",
    "        print(f\"No watermark record found for {table_name}. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    last_watermark = watermark_df.collect()[0][\"LastWatermarkValue\"]\n",
    "    incremental_column_ds = watermark_df.collect()[0][\"IncrementalColumn\"]\n",
    "\n",
    "    print(f\"ðŸ•“ Last Watermark for {table_name}: {last_watermark}\")\n",
    "    print(f\"ðŸ“… Incremental Column: {incremental_column_ds}\")\n",
    "\n",
    "    # Step 2 â€” Read Delta Table\n",
    "    try:\n",
    "        df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Delta table for {table_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    if incremental_column_ds not in df_silver.columns:\n",
    "        print(f\" Skipping {table_name}: Missing '{incremental_column_ds}' column.\")\n",
    "        continue\n",
    "\n",
    "    #  Step 3 â€” Find Max Timestamp\n",
    "    max_cleaned_ts = df_silver.agg(F.max(F.col(incremental_column_ds))).collect()[0][0]\n",
    "    if not max_cleaned_ts:\n",
    "        print(f\"No valid '{incremental_column_ds}' found for {table_name}.\")\n",
    "        continue\n",
    "\n",
    "    #  Convert max_cleaned_ts to SQL-compatible string\n",
    "    if isinstance(max_cleaned_ts, datetime):\n",
    "        max_cleaned_ts_str = max_cleaned_ts.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    else:\n",
    "        max_cleaned_ts_str = str(max_cleaned_ts)[:19]  # fallback\n",
    "\n",
    "    #  Step 4 â€” Filter Incremental Records\n",
    "    df_incremental = df_silver.filter(\n",
    "        (F.col(incremental_column_ds) > F.to_timestamp(F.lit(last_watermark)))\n",
    "        & (F.col(incremental_column_ds) <= F.lit(max_cleaned_ts))\n",
    "    )\n",
    "\n",
    "    incr_count = df_incremental.count()\n",
    "    print(f\" Incremental Records Found: {incr_count}\")\n",
    "\n",
    "    if incr_count == 0:\n",
    "        print(f\"No new data for {table_name}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    #  Step 5 â€” Write Incremental Data to SQL\n",
    "    target_table = f\"dbo.{table_name}_Prestage\"\n",
    "    try:\n",
    "        df_incremental.write.format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", target_table) \\\n",
    "            .option(\"user\", sql_user) \\\n",
    "            .option(\"password\", sql_password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .save()\n",
    "\n",
    "        print(f\" {incr_count} records written to {target_table}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed writing {table_name} to SQL: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    #  Step 6 â€” Update Watermark using JVM JDBC (fixed timestamp format)\n",
    "    try:\n",
    "        conn = spark._sc._gateway.jvm.java.sql.DriverManager.getConnection(\n",
    "            jdbc_url, sql_user, sql_password\n",
    "        )\n",
    "        stmt = conn.createStatement()\n",
    "\n",
    "        update_sql = f\"\"\"\n",
    "        UPDATE dbo.configtable\n",
    "        SET LastWatermarkValue = CONVERT(datetime, '{max_cleaned_ts_str}', 120)\n",
    "        WHERE SourceObjectName = '{table_name}' AND DestinationZone = 'Goldprestage'\n",
    "        \"\"\"\n",
    "\n",
    "        stmt.executeUpdate(update_sql)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print(f\" Watermark updated for {table_name} â†’ {max_cleaned_ts_str}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Failed to update watermark for {table_name}: {str(e)}\")\n",
    "\n",
    "# ---------------------------------------------\n",
    "#  Process Full Load Tables (Overwrite Mode)\n",
    "# ---------------------------------------------\n",
    "for table_name in full_load_tables:\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\" Full Load: {table_name}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    silver_path = f\"{silver_base_path}/{table_name}\"\n",
    "\n",
    "    try:\n",
    "        df_silver = spark.read.format(\"delta\").load(silver_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading Delta table for {table_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "    record_count = df_silver.count()\n",
    "    print(f\" Total Records to Load: {record_count}\")\n",
    "\n",
    "    target_table = f\"dbo.{table_name}_Prestage\"\n",
    "\n",
    "    try:\n",
    "        df_silver.write.format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", target_table) \\\n",
    "            .option(\"user\", sql_user) \\\n",
    "            .option(\"password\", sql_password) \\\n",
    "            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .save()\n",
    "\n",
    "        print(f\"Full load completed for {table_name}: {record_count} records written.\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error writing full load for {table_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\n All Silver tables processed successfully (Incremental + Full loads)!\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "SilverNoteBook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}