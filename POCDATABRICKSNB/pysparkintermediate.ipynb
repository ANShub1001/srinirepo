{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc86b315-f2b6-475b-a27c-8c95008410a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " dbutils.fs.mount(\n",
    "     source='wasbs://demo1@datastorage123.blob.core.windows.net/',\n",
    "     mount_point='/mnt/srini',\n",
    "     extra_configs={\n",
    " 'fs.azure.account.key.datastorage123.blob.core.windows.net': '6xEUxRWroFePxZjTYSsjxbm0OQXz4/T67SJ+36IQziH7RuvG1XY+JIK9QLskt7Y2Ovi4lRXp2BpY+AStiKJgPQ=='\n",
    "    }\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4462f29e-7338-4cd0-968a-671b5230fe51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+\n| id|   name|department|salary|\n+---+-------+----------+------+\n|  1|  Alice|     Sales|  5000|\n|  2|    Bob|     Sales|  6000|\n|  3|Charlie|     Sales|  6000|\n|  4|  David|        HR|  4000|\n|  5|    Eve|        HR|  4000|\n|  6|  Frank|        HR|  4500|\n|  7|  Grace|        IT|  7500|\n|  8|  Heidi|        IT|  7200|\n|  9|   Ivan|        IT|  7200|\n| 10|   Judy|   Finance|  6700|\n| 11|  Kevin|   Finance|  6700|\n| 12|  Laura|   Finance|  6200|\n| 13|Mallory|     Sales|  5200|\n| 14|   Neil|     Sales|  5200|\n| 15|  Oscar|     Sales|  4800|\n| 16|  Peggy|        HR|  4500|\n| 17|Quentin|        HR|  4100|\n| 18|   Ruth|   Finance|  5900|\n| 19|  Steve|        IT|  7000|\n| 20|  Trent|   Finance|  6300|\n+---+-------+----------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.datastorage123.dfs.core.windows.net\",\n",
    "  \"6xEUxRWroFePxZjTYSsjxbm0OQXz4/T67SJ+36IQziH7RuvG1XY+JIK9QLskt7Y2Ovi4lRXp2BpY+AStiKJgPQ==\"\n",
    ")\n",
    " \n",
    "df = spark.read.csv(\"abfss://demo1@datastorage123.dfs.core.windows.net/Databrickssampledata.csv\", header=True)\n",
    "df.show()\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47203ba8-80ac-4ddc-917d-91b3f2b31e80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----------+------+-------+----+----------+-------+\n| id|   name|department|salary|row_num|rank|dense_rank|ntile_2|\n+---+-------+----------+------+-------+----+----------+-------+\n| 10|   Judy|   Finance|  6700|      1|   1|         1|      1|\n| 11|  Kevin|   Finance|  6700|      2|   1|         1|      1|\n| 20|  Trent|   Finance|  6300|      3|   3|         2|      1|\n| 12|  Laura|   Finance|  6200|      4|   4|         3|      2|\n| 18|   Ruth|   Finance|  5900|      5|   5|         4|      2|\n|  6|  Frank|        HR|  4500|      1|   1|         1|      1|\n| 16|  Peggy|        HR|  4500|      2|   1|         1|      1|\n| 17|Quentin|        HR|  4100|      3|   3|         2|      1|\n|  4|  David|        HR|  4000|      4|   4|         3|      2|\n|  5|    Eve|        HR|  4000|      5|   4|         3|      2|\n|  7|  Grace|        IT|  7500|      1|   1|         1|      1|\n|  8|  Heidi|        IT|  7200|      2|   2|         2|      1|\n|  9|   Ivan|        IT|  7200|      3|   2|         2|      2|\n| 19|  Steve|        IT|  7000|      4|   4|         3|      2|\n|  2|    Bob|     Sales|  6000|      1|   1|         1|      1|\n|  3|Charlie|     Sales|  6000|      2|   1|         1|      1|\n| 13|Mallory|     Sales|  5200|      3|   3|         2|      1|\n| 14|   Neil|     Sales|  5200|      4|   3|         2|      1|\n|  1|  Alice|     Sales|  5000|      5|   5|         3|      2|\n| 21|    Uma|     Sales|  4900|      6|   6|         4|      2|\n+---+-------+----------+------+-------+----+----------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank, dense_rank, ntile\n",
    "\n",
    "# Read the CSV with inferred schema\n",
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"abfss://demo1@datastorage123.dfs.core.windows.net/Databrickssampledata.csv\"\n",
    ")\n",
    "\n",
    "# Define window specification\n",
    "window_spec = Window.partitionBy(\"department\").orderBy(df.salary.desc())\n",
    "\n",
    "# Apply window functions\n",
    "df_with_ranks = df.select(\n",
    "    \"*\",\n",
    "    row_number().over(window_spec).alias(\"row_num\"),\n",
    "    rank().over(window_spec).alias(\"rank\"),\n",
    "    dense_rank().over(window_spec).alias(\"dense_rank\"),\n",
    "    ntile(2).over(window_spec).alias(\"ntile_2\")  # Split each department into 2 tiles\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "df_with_ranks.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0550c443-5d6b-4234-b183-1f446bc828c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----+----+-----+\n|   name|Finance|  HR|  IT|Sales|\n+-------+-------+----+----+-----+\n|  Grace|   null|null|7500| null|\n|  Steve|   null|null|7000| null|\n|  Trent|   6300|null|null| null|\n|Quentin|   null|4100|null| null|\n|  Heidi|   null|null|7200| null|\n|Charlie|   null|null|null| 6000|\n|   Judy|   6700|null|null| null|\n|   Neil|   null|null|null| 5200|\n|    Bob|   null|null|null| 6000|\n|Mallory|   null|null|null| 5200|\n|  Oscar|   null|null|null| 4800|\n|  Alice|   null|null|null| 5000|\n|    Eve|   null|4000|null| null|\n|  Kevin|   6700|null|null| null|\n|   Ruth|   5900|null|null| null|\n|  David|   null|4000|null| null|\n|    Uma|   null|null|null| 4900|\n|   Ivan|   null|null|7200| null|\n|  Peggy|   null|4500|null| null|\n|  Frank|   null|4500|null| null|\n+-------+-------+----+----+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "pivot_df = df.groupBy(\"name\").pivot(\"department\").sum(\"salary\")\n",
    "pivot_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370bfab4-a4c1-4c73-bc9e-8d9dff9d1568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n|   name|department|salary|\n+-------+----------+------+\n|  Grace|        IT|  7500|\n|  Steve|        IT|  7000|\n|Quentin|        HR|  4100|\n|  Heidi|        IT|  7200|\n|Charlie|     Sales|  6000|\n|   Neil|     Sales|  5200|\n|    Bob|     Sales|  6000|\n|Mallory|     Sales|  5200|\n|  Oscar|     Sales|  4800|\n|  Alice|     Sales|  5000|\n|    Eve|        HR|  4000|\n|  David|        HR|  4000|\n|    Uma|     Sales|  4900|\n|   Ivan|        IT|  7200|\n|  Peggy|        HR|  4500|\n|  Frank|        HR|  4500|\n+-------+----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Unpivot using stack()\n",
    "unpivot_df = pivot_df.selectExpr(\n",
    "    \"name\",\n",
    "    \"stack(3, 'HR', HR, 'Sales', Sales, 'IT', IT) as (department, salary)\"\n",
    ").where(\"salary is not null\")\n",
    "\n",
    "unpivot_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bc48c82-9052-4fc5-a2a7-fbd671ff2601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n| id| name| skill|\n+---+-----+------+\n|  1|Alice|   SQL|\n|  1|Alice|Python|\n|  1|Alice| Spark|\n+---+-----+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import Row\n",
    "\n",
    "data = [Row(id=1, name=\"Alice\", skills=[\"SQL\", \"Python\", \"Spark\"])]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df_exploded = df.select(\"id\", \"name\", explode(\"skills\").alias(\"skill\"))\n",
    "df_exploded.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d5edf99-7639-45b0-bfb2-240926ab709c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+\n|name|zipped                     |\n+----+---------------------------+\n|Bob |[{Math, 90}, {English, 85}]|\n+----+---------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import arrays_zip\n",
    "\n",
    "data = [\n",
    "    Row(name=\"Bob\", subjects=[\"Math\", \"English\"], scores=[90, 85])\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "df_zipped = df.select(\"name\", arrays_zip(\"subjects\", \"scores\").alias(\"zipped\"))\n",
    "df_zipped.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1a38ca9-02d2-4747-9fb9-ab3c2e9d2368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+\n|name|subjects|scores|\n+----+--------+------+\n| Bob|    Math|    90|\n| Bob| English|    85|\n+----+--------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_flattened = df_zipped.select(\"name\", explode(\"zipped\").alias(\"subject_score\"))\n",
    "df_flattened.select(\n",
    "    \"name\",\n",
    "    \"subject_score.subjects\",\n",
    "    \"subject_score.scores\"\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9491cc65-d4fd-43f2-ae55-2c9b171a8eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n*(1) Filter (isnotnull(salary#70) AND (salary#70 > 6000))\n+- FileScan csv [id#67,name#68,department#69,salary#70] Batched: false, DataFilters: [isnotnull(salary#70), (salary#70 > 6000)], Format: CSV, Location: InMemoryFileIndex(1 paths)[abfss://demo1@datastorage123.dfs.core.windows.net/Databrickssampledata..., PartitionFilters: [], PushedFilters: [IsNotNull(salary), GreaterThan(salary,6000)], ReadSchema: struct<id:int,name:string,department:string,salary:int>\n\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\n",
    "    \"abfss://demo1@datastorage123.dfs.core.windows.net/Databrickssampledata.csv\"\n",
    ")\n",
    "df_filtered = df.filter(df.salary > 6000)\n",
    "df_filtered.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac7233ce-46e0-4901-b27a-13424295fc07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Aggregate ['department], ['department, avg(salary#70) AS avg(salary)#81]\n+- Filter (salary#70 > 5000)\n   +- Relation [id#67,name#68,department#69,salary#70] csv\n\n== Analyzed Logical Plan ==\ndepartment: string, avg(salary): double\nAggregate [department#69], [department#69, avg(salary#70) AS avg(salary)#81]\n+- Filter (salary#70 > 5000)\n   +- Relation [id#67,name#68,department#69,salary#70] csv\n\n== Optimized Logical Plan ==\nAggregate [department#69], [department#69, avg(salary#70) AS avg(salary)#81]\n+- Project [department#69, salary#70]\n   +- Filter (isnotnull(salary#70) AND (salary#70 > 5000))\n      +- Relation [id#67,name#68,department#69,salary#70] csv\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- HashAggregate(keys=[department#69], functions=[finalmerge_avg(merge sum#86, count#87L) AS avg(salary#70)#80], output=[department#69, avg(salary)#81])\n   +- Exchange hashpartitioning(department#69, 200), ENSURE_REQUIREMENTS, [plan_id=71]\n      +- HashAggregate(keys=[department#69], functions=[partial_avg(salary#70) AS (sum#86, count#87L)], output=[department#69, sum#86, count#87L])\n         +- Filter (isnotnull(salary#70) AND (salary#70 > 5000))\n            +- FileScan csv [department#69,salary#70] Batched: false, DataFilters: [isnotnull(salary#70), (salary#70 > 5000)], Format: CSV, Location: InMemoryFileIndex(1 paths)[abfss://demo1@datastorage123.dfs.core.windows.net/Databrickssampledata..., PartitionFilters: [], PushedFilters: [IsNotNull(salary), GreaterThan(salary,5000)], ReadSchema: struct<department:string,salary:int>\n\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df.filter(\"salary > 5000\").groupBy(\"department\").avg(\"salary\")\n",
    "df_filtered.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e3445b-9d8d-4341-847b-52d2f62b8589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_repartitioned = df.repartition(10)  # creates 10 partitions\n",
    "df_repartitioned = df.repartition(\"department\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22385a9b-3e68-47ae-9b63-8ac815063ffc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_coalesced = df.coalesce(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c67891dd-5bd3-4046-b6b0-32c8d3849f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write \\\n",
    "  .format(\"parquet\") \\\n",
    "  .bucketBy(5, \"department\") \\\n",
    "  .sortBy(\"id\") \\\n",
    "  .saveAsTable(\"bucketed_table\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pysparkintermediate",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}